#!/usr/bin/env python3
"""
Simple training script for building detection using prepared dataset.
"""

import sys
from pathlib import Path

# Add project root to path
sys.path.append(str(Path(__file__).parent))

from config import config, DatasetConfig, TrainingConfig
from model_wrapper import ModelWrapper
from trainer import Trainer
from evaluator import Evaluator

def main():
    from datetime import datetime
    
    start_time = datetime.now()
    print("\n" + "="*60)
    print("üèóÔ∏è  BUILDING DETECTION TRAINING")
    print("="*60)
    print(f"Started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)
    
    # Configure dataset for buildings - ATTEMPT MULTI-DATASET IF AVAILABLE
    print("\nüìÅ Configuring dataset - checking for multiple datasets...")
    
    # Check what datasets are available
    import json
    import glob
    
    available_datasets = []
    dataset_configs = glob.glob("prepared_datasets/*/dataset_info.json")
    
    for config_file in dataset_configs:
        try:
            with open(config_file, 'r') as f:
                dataset_info = json.load(f)
            if 'clean' in dataset_info['name'] or dataset_info.get('overlap', 128) <= 32:
                available_datasets.append(dataset_info)
        except:
            continue
    
    if len(available_datasets) > 1:
        print(f"üöÄ Found {len(available_datasets)} diverse datasets - using MULTI-DATASET approach!")
        
        # Create combined IDs from all available clean datasets
        all_train_ids = []
        all_test_ids = []
        for i, dataset_info in enumerate(available_datasets):
            dataset_name = dataset_info['name']
            print(f"   ‚Ä¢ {dataset_name}: {len(dataset_info['train_ids'])} train, {len(dataset_info['test_ids'])} test")
            
            # Add train IDs with dataset prefix
            for train_id in dataset_info['train_ids']:
                clean_id = train_id.replace('img_', '')
                prefixed_id = f"{dataset_name}_{clean_id}"
                all_train_ids.append(prefixed_id)
            
            # Add test IDs with dataset prefix
            for test_id in dataset_info['test_ids']:
                clean_id = test_id.replace('img_', '')
                prefixed_id = f"{dataset_name}_{clean_id}"
                all_test_ids.append(prefixed_id)
        
        # Create dataset mapping for multi-dataset loader
        dataset_mapping = {}
        for dataset_info in available_datasets:
            dataset_path = f"prepared_datasets/{dataset_info['name']}"
            for train_id in dataset_info['train_ids']:
                clean_id = train_id.replace('img_', '')
                prefixed_id = f"{dataset_info['name']}_{clean_id}"
                dataset_mapping[prefixed_id] = {
                    'dataset_path': dataset_path,
                    'original_id': clean_id
                }
            for test_id in dataset_info['test_ids']:
                clean_id = test_id.replace('img_', '')
                prefixed_id = f"{dataset_info['name']}_{clean_id}"
                dataset_mapping[prefixed_id] = {
                    'dataset_path': dataset_path,
                    'original_id': clean_id
                }
        
        # Save mapping for multi-dataset loader
        with open("multi_dataset_mapping.json", 'w') as f:
            json.dump(dataset_mapping, f, indent=2)
        
        buildings_dataset = DatasetConfig(
            name="multi_dataset_combined",
            train_ids=all_train_ids,
            test_ids=all_test_ids,
            stride_size=64,
            epochs=25,   # More epochs with diverse multi-dataset
            save_epoch=3,
            data_root="./prepared_datasets",
            data_pattern="images/img_{}.png",
            dsm_pattern="dsm/dsm_{}.tif", 
            label_pattern="labels/label_{}.png"
        )
        print(f"‚úÖ MULTI-DATASET configuration: {len(all_train_ids)} train + {len(all_test_ids)} test samples!")
        
    else:
        print("üìÅ Using single dataset approach (buildings_clean)")
        buildings_dataset = DatasetConfig(
            name="buildings_clean",
            train_ids=[f"{i:06d}" for i in range(667)],  # 667 diverse training tiles (32px overlap)
            test_ids=[f"{i:06d}" for i in range(667, 785)],  # 118 test tiles 
            stride_size=64,
            epochs=20,   # More epochs with cleaner data
            save_epoch=3,
            data_root="./prepared_datasets/buildings_clean",
            data_pattern="images/img_{}.png",  # This will add "img_" prefix
            dsm_pattern="dsm/dsm_{}.tif", 
            label_pattern="labels/label_{}.png"
        )
    print("‚úÖ Dataset configuration created")
    
    # Configure training - BALANCED SETTINGS FOR LARGER DATASET
    print("\n‚öôÔ∏è  Configuring BALANCED training parameters for massive dataset...")
    training_config = TrainingConfig(
        base_lr=0.0001,         # Moderate learning rate (2x higher due to more data)
        weight_decay=0.005,     # Moderate regularization (5x increase from default)
        batch_size=6,           # Slightly larger batches due to more data
        momentum=0.9,           # Standard momentum
        scheduler_milestones=[8, 12],  # Later decay due to more epochs
        scheduler_gamma=0.5,    # Moderate LR decay (50% reduction)
        window_size=(256, 256),
        n_classes=2,            # Buildings: background + buildings
        cache=True
    )
    
    # Apply configurations
    config.training = training_config
    config.set_dataset_config(buildings_dataset)
    print("‚úÖ Training configuration updated for clean diverse dataset")
    
    # Print configuration
    if config.dataset.name == "multi_dataset_combined":
        print("\nüìä üöÄ MULTI-DATASET Configuration Summary:")
        print(f"   Dataset:       {config.dataset.name}")
        print(f"   Classes:       {config.n_classes} ({', '.join(config.labels)})")
        print(f"   Training tiles:{len(config.dataset.train_ids):4d} (ULTIMATE DIVERSITY!)")
        print(f"   Test tiles:    {len(config.dataset.test_ids):4d}")
        print(f"   Source datasets: {len(available_datasets)}")
        print(f"   Data root:     {config.dataset.data_root}")
        print(f"   Epochs:        {config.dataset.epochs}")
        print(f"   Validation:    every {config.dataset.save_epoch} epochs")
        print("\n‚öôÔ∏è  MULTI-DATASET Training Parameters:")
        print(f"   Learning rate: {config.training.base_lr:.6f} (moderate)")
        print(f"   Weight decay:  {config.training.weight_decay:.3f} (balanced regularization)")
        print(f"   Batch size:    {config.training.batch_size} (moderate)")
        print(f"   Momentum:      {config.training.momentum:.1f} (standard)")
        print(f"   LR schedule:   {config.training.scheduler_milestones} (reasonable)")
        print(f"   LR decay:      {config.training.scheduler_gamma:.1f} (50% reduction)")
        print("\nüéØ ULTIMATE OVERFITTING PREVENTION:")
        print("   ‚Ä¢ Multiple geographic regions (MOPR + Aarvi)")
        print("   ‚Ä¢ Different building styles and environments")
        print("   ‚Ä¢ Diverse lighting and seasonal conditions")
        print("   ‚Ä¢ Low overlap tiles (32px max)")
        print("   ‚Ä¢ Aggressive data augmentation")
        print("   ‚Ä¢ THIS SHOULD COMPLETELY SOLVE OVERFITTING!")
    else:
        print("\nüìä SINGLE DATASET Configuration Summary:")
        print(f"   Dataset:       {config.dataset.name}")
        print(f"   Classes:       {config.n_classes} ({', '.join(config.labels)})")
        print(f"   Training tiles:{len(config.dataset.train_ids):4d} (DIVERSE - 32px overlap only)")
        print(f"   Test tiles:    {len(config.dataset.test_ids):4d}")
        print(f"   Data root:     {config.dataset.data_root}")
        print(f"   Epochs:        {config.dataset.epochs}")
        print(f"   Validation:    every {config.dataset.save_epoch} epochs")
        print("\n‚öôÔ∏è  BALANCED Training Parameters + Aggressive Augmentation:")
        print(f"   Learning rate: {config.training.base_lr:.6f} (moderate)")
        print(f"   Weight decay:  {config.training.weight_decay:.3f} (balanced regularization)")
        print(f"   Batch size:    {config.training.batch_size} (moderate)")
        print(f"   Momentum:      {config.training.momentum:.1f} (standard)")
        print(f"   LR schedule:   {config.training.scheduler_milestones} (reasonable)")
        print(f"   LR decay:      {config.training.scheduler_gamma:.1f} (50% reduction)")
        print("\nüéØ EXPECTED: BREAKTHROUGH with diverse tiles + augmentation!")
        print("   ‚Ä¢ Training accuracy: Gradual rise 40% ‚Üí 70% ‚Üí 85%")
        print("   ‚Ä¢ NO tile similarity overfitting")
        print("   ‚Ä¢ Much healthier learning curves")
        print("   ‚Ä¢ Each tile is 87% MORE DIVERSE than before")
    
    # Initialize model
    print("\nüß† Initializing UNetFormer model...")
    model_wrapper = ModelWrapper(num_classes=config.n_classes)
    model_wrapper.print_parameter_summary()
    
    # Initialize evaluator
    print("\nüîç Initializing evaluator...")
    evaluator = Evaluator()
    print("‚úÖ Evaluator ready")
    
    # Initialize trainer
    print("\nüèÉ Initializing trainer...")
    trainer = Trainer(model_wrapper, evaluator)
    print("‚úÖ Trainer ready")
    
    try:
        if config.dataset.name == "multi_dataset_combined":
            print("\n" + "="*70)
            print("üöÄ TRAINING WITH ULTIMATE MULTI-DATASET DIVERSITY")
            print("="*70)
            print("‚ö° ULTIMATE OVERFITTING PREVENTION STRATEGY:")
            print("   ‚Ä¢ Updates every 10 batches")
            print("   ‚Ä¢ Real-time trend indicators (‚Üë‚Üì‚Üí)")
            print("   ‚Ä¢ Running averages for loss and accuracy")
            print(f"   ‚Ä¢ {len(config.dataset.train_ids)} ULTRA-DIVERSE tiles from multiple regions")
            print("   ‚Ä¢ AGGRESSIVE data augmentation per sample")
            print("   ‚Ä¢ Validation every 3 epochs")
            print("")
            print("‚úÖ EXPECTED BREAKTHROUGH (multi-dataset diversity):")
            print("   ‚Ä¢ Training accuracy: VERY SLOW gradual rise 40% ‚Üí 70% ‚Üí 85%")
            print("   ‚Ä¢ NO immediate high accuracy")
            print("   ‚Ä¢ ROCK SOLID learning curves")
            print("   ‚Ä¢ NO geographic overfitting")
            print("   ‚Ä¢ EXCELLENT generalization")
            print("")
            print("üéØ DIVERSITY POWER:")
            print("   ‚Ä¢ Multiple geographic regions prevent location bias")
            print("   ‚Ä¢ Different building styles and environments")
            print("   ‚Ä¢ Various lighting and seasonal conditions")
            print("   ‚Ä¢ Maximum dataset diversity achieved!")
            print("")
            print("üî• THIS SHOULD FINALLY SOLVE OVERFITTING COMPLETELY!")
            print("="*70)
        else:
            print("\n" + "="*60)
            print("üöÄ TRAINING WITH CLEAN DIVERSE DATASET")
            print("="*60)
            print("‚ö° BREAKTHROUGH STRATEGY:")
            print("   ‚Ä¢ Updates every 10 batches")
            print("   ‚Ä¢ Real-time trend indicators (‚Üë‚Üì‚Üí)")
            print("   ‚Ä¢ Running averages for loss and accuracy")
            print("   ‚Ä¢ 667 DIVERSE tiles (32px overlap vs 128px)")
            print("   ‚Ä¢ Aggressive data augmentation")
            print("   ‚Ä¢ Validation every 3 epochs")
            print("")
            print("‚úÖ EXPECTED HEALTHY SIGNS (diverse tiles):")
            print("   ‚Ä¢ Training accuracy: SLOW gradual rise 40% ‚Üí 70% ‚Üí 85%")
            print("   ‚Ä¢ NO immediate 90%+ accuracy")
            print("   ‚Ä¢ Much more stable learning curves")
            print("   ‚Ä¢ No tile similarity memorization")
            print("")
            print("üéØ OVERFITTING SOLUTION:")
            print("   ‚Ä¢ 87% LESS tile similarity (32px vs 128px overlap)")
            print("   ‚Ä¢ Each tile much more unique")
            print("   ‚Ä¢ Aggressive augmentation creates diversity")
            print("   ‚Ä¢ Should finally see healthy learning!")
            print("")
            print("üî• KEY: DIVERSE TILES + AUGMENTATION = NO OVERFITTING")
            print("="*60)
        
        trainer.train(epochs=config.dataset.epochs)
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        print("\n" + "="*60)
        print("üéâ TRAINING COMPLETED SUCCESSFULLY!")
        print("="*60)
        print(f"Completed at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Total time:   {str(duration).split('.')[0]}")
        print("="*60)
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Training interrupted by user")
        print("Partial results may be saved")
    except Exception as e:
        print(f"\n‚ùå Training error: {e}")
        raise

if __name__ == "__main__":
    main()
