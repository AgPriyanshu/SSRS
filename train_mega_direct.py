#!/usr/bin/env python3
"""
Direct MEGA Multiscale Training Script - No Inspection, Straight to Training!

This script starts training immediately with the MEGA multiscale dataset.
"""

import sys
from pathlib import Path
from datetime import datetime
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

# Add project root to path
sys.path.append(str(Path(__file__).parent))

from config import config, DatasetConfig, TrainingConfig
from model_wrapper import ModelWrapper
from trainer import Trainer
from evaluator import Evaluator
from multiscale_dataset import create_multiscale_datasets


def main():
    """Direct training with MEGA multiscale dataset."""
    
    start_time = datetime.now()
    print("="*80)
    print("üöÄ MEGA MULTISCALE TRAINING - DIRECT START")
    print("="*80)
    print(f"Started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)
    
    # MEGA dataset configuration
    mega_dataset = DatasetConfig(
        name="MEGA_multiscale",
        train_ids=[],  # Will be loaded from dataset_info.json
        test_ids=[],   # Will be loaded from dataset_info.json
        stride_size=64,
        epochs=30,     # More epochs for the massive dataset
        save_epoch=2,  # Save every 2 epochs
        data_root="./prepared_datasets/MEGA_multiscale",
        data_pattern="images/img_{}.png",
        dsm_pattern="dsm/dsm_{}.tif",
        label_pattern="labels/label_{}.png"
    )
    
    # Enhanced training configuration for multiscale learning
    training_config = TrainingConfig(
        base_lr=0.0001,         # Conservative learning rate for massive dataset
        weight_decay=0.01,      # Strong regularization for diverse data
        batch_size=4,           # Smaller batches due to multiscale complexity
        momentum=0.9,
        scheduler_milestones=[10, 20, 25],  # Progressive LR decay
        scheduler_gamma=0.5,    # 50% LR reduction at milestones
        window_size=(256, 256),
        n_classes=2,            # Buildings: background + buildings
        cache=False             # Don't cache due to large dataset size
    )
    
    # Apply configurations
    config.training = training_config
    config.set_dataset_config(mega_dataset)
    
    print(f"\nüìä MEGA DATASET CONFIGURATION:")
    print(f"   Dataset name: {mega_dataset.name}")
    print(f"   Data root: {mega_dataset.data_root}")
    print(f"   Epochs: {mega_dataset.epochs}")
    print(f"   Save every: {mega_dataset.save_epoch} epochs")
    
    print(f"\n‚öôÔ∏è  OPTIMIZED TRAINING PARAMETERS:")
    print(f"   Learning rate: {training_config.base_lr:.6f}")
    print(f"   Weight decay: {training_config.weight_decay:.3f}")
    print(f"   Batch size: {training_config.batch_size}")
    print(f"   Momentum: {training_config.momentum:.1f}")
    print(f"   LR milestones: {training_config.scheduler_milestones}")
    print(f"   LR decay: {training_config.scheduler_gamma:.1f}")
    
    # Create data loaders
    print(f"\nüìä Creating MEGA multiscale data loaders...")
    
    # Create datasets using our multiscale loader
    train_dataset, test_dataset = create_multiscale_datasets(
        dataset_info_file="prepared_datasets/MEGA_multiscale/dataset_info.json",
        window_size=(256, 256),
        stride_size=64,
        augment_train=True,
        cache=False,  # Don't cache due to large size
        debug=False   # No debug output for direct training
    )
    
    print(f"‚úÖ Datasets loaded:")
    print(f"   Training patches: {len(train_dataset):,}")
    print(f"   Testing patches:  {len(test_dataset):,}")
    
    # Create data loaders with optimized settings
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.training.batch_size,
        shuffle=True,
        num_workers=2,  # Reduced workers for stability
        pin_memory=True,
        drop_last=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.training.batch_size,
        shuffle=False,
        num_workers=2,  # Reduced workers for stability
        pin_memory=True
    )
    
    print(f"‚úÖ Data loaders ready:")
    print(f"   Train batches: {len(train_loader):,}")
    print(f"   Test batches:  {len(test_loader):,}")
    
    # Initialize model
    print(f"\nüß† Initializing UNetFormer + Deep Fusion Module...")
    model_wrapper = ModelWrapper(num_classes=config.n_classes)
    model_wrapper.print_parameter_summary()
    
    # Initialize evaluator
    print(f"\nüîç Initializing evaluator...")
    evaluator = Evaluator()
    print("‚úÖ Evaluator ready")
    
    # Initialize trainer with custom data loaders
    print(f"\nüèÉ Initializing trainer with MEGA multiscale data...")
    trainer = Trainer(model_wrapper, evaluator)
    
    # Override trainer's data loaders with our multiscale ones
    trainer.train_loader = train_loader
    trainer.test_loader = test_loader
    
    # Disable mini-validation for multiscale training (since we use custom loaders)
    trainer.config.dataset.test_ids = ["dummy_test_id"]  # Prevent empty validation
    
    print("‚úÖ Trainer ready with multiscale data loaders")
    
    try:
        print(f"\n" + "="*80)
        print("üöÄ STARTING MEGA MULTISCALE TRAINING NOW!")
        print("="*80)
        print("‚ö° ULTIMATE TRAINING CONFIGURATION:")
        print(f"   ‚Ä¢ {len(train_dataset):,} training patches from 4 scale levels")
        print(f"   ‚Ä¢ Perfect 25% distribution across all scales")
        print(f"   ‚Ä¢ 2 geographic regions (MOPR + Aarvi)")
        print(f"   ‚Ä¢ Scale-aware data augmentation")
        print(f"   ‚Ä¢ Deep Fusion Module architecture")
        print(f"   ‚Ä¢ Conservative LR with progressive decay")
        print("")
        print("‚ú® EXPECTED BREAKTHROUGH RESULTS:")
        print("   ‚Ä¢ Gradual accuracy improvement (no overfitting)")
        print("   ‚Ä¢ Excellent multi-scale feature learning")
        print("   ‚Ä¢ Superior cross-region generalization")
        print("   ‚Ä¢ State-of-the-art building detection")
        print("")
        print("üî• THIS IS THE ULTIMATE DATASET FOR YOUR DEEP FUSION MODULE!")
        print("="*80)
        
        # Start training immediately
        trainer.train(epochs=mega_dataset.epochs)
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        print(f"\n" + "="*80)
        print("üéâ MEGA MULTISCALE TRAINING COMPLETED SUCCESSFULLY!")
        print("="*80)
        print(f"Completed at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Total time:   {str(duration).split('.')[0]}")
        print(f"Dataset used: MEGA multiscale with {len(train_dataset):,} training patches")
        print(f"Scale levels: 4 (1x, 2x, 4x, 8x)")
        print(f"Geographic regions: 2 (MOPR + Aarvi)")
        print("="*80)
        
    except KeyboardInterrupt:
        print(f"\n‚ö†Ô∏è  Training interrupted by user")
        print("Partial results may be saved")
    except Exception as e:
        print(f"\n‚ùå Training error: {e}")
        raise


if __name__ == "__main__":
    main()
